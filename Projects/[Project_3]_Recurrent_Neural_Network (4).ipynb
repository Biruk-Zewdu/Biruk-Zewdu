{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sisrel/EE214-Practice-Materials/blob/main/Practice-Fall2023/%5BProject_3%5D_Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Project 3] Recurrent Neural Networks\n",
        "\n",
        "This project consists of two parts. In the first half, we will try to **implement multiple types of RNN from scratch**. In the second half, we will actually **train an RNN and test** how well they work.  \n",
        "\n",
        "Below are the detailed contents for this project:  \n",
        "\n",
        "  * Part 1 (25 pts). Multiple types of RNN\n",
        "  * Part 2 (30 pts). Enigma decrypt: Classifier  \n",
        "  * Part 3 (30 pts). Enigma decrypt: Decipherer\n",
        "  * Part 4 (15 pts). Enigma decryptor"
      ],
      "metadata": {
        "id": "U5FRNDgsWtXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Important**  \n",
        "* You have to write some descriptions of codes and results for your points in each part. For required sections, we will make the **markdown** box to write in and inform you what to write. Check **ALL** steps that have points.  \n",
        "* Training takes a few hours. We recommend you start early.  Also, use your time wisely, e.g., check for errors by executing the cells individually before running the entire code.\n",
        "* We highly recommend you to **use GPU**. (`runtime` > `runtime type change` > `GPU`)\n",
        "* **Submit** the completed Colab file and its pdf version on the KLMS. Their file names should be:  \n",
        "  * **[STUDENT_ID]_project3.ipynb**, e.g., 20XXXXXX_project3.ipynb\n",
        "  * **[STUDENT_ID]_project3.pdf**"
      ],
      "metadata": {
        "id": "KOVOCQO25gGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Part 1 (Total 25 pts)**. Multiple types of RNN  \n",
        "\n",
        "In the part 1-6, you will test all your models. You should get 27 passes to get max points, as shown below.  \n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/pass.JPG\" width=\"70%\"/>\n",
        "<br/>\n",
        "</p>\n",
        "\n",
        "If you have made some mistakes, you might get a result like the figure below. `F` indicates failed task, and `.` indicates passed task.  \n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/fail.JPG\" width=\"70%\"/>\n",
        "<br/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "0-ijaLup6TuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an RNN"
      ],
      "metadata": {
        "id": "lUFMglcvPC0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1-1 (Total 5 pts). Recurrent Neural Network\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/LSTM3-SimpleRNN.png\" width=\"70%\"/>\n",
        "<br/>\n",
        "<em>Figure from <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>'s blog <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">post</a>.</em>\n",
        "</p>\n",
        "\n",
        "Recall the update equation for an RNN.\n",
        "\n",
        "$$h^{(t)}=\\tanh(Wh^{(t-1)}+Ux^{(t)}+b)$$\n",
        "\n",
        "You have **two tasks** to do:  \n",
        "\n",
        "1. **Complete the skeleton code** given below using the equation above. You only need to rewrite the **`forward`** method. **Do not modify the methods `__init__` and `copy_from_torch`**. Feel free to implement additional helper functions if you need them.\n",
        "2. Explain how RNN works **briefly** (e.g., meaning of the update equations, role of intermediate variables)"
      ],
      "metadata": {
        "id": "ChPmYLwIsRxC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdUOGCVWWnh5"
      },
      "outputs": [],
      "source": [
        "%%file my_rnn_cell.py\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "\n",
        "class MyRNNCell(nn.Module):\n",
        "    \"\"\" An RNN cell with tanh nonlinearity\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.U = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.b = nn.Parameter(torch.empty((hidden_size,)))\n",
        "\n",
        "    def forward(self, x: Tensor, h: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the next hidden state.\n",
        "\n",
        "        Parameters:\n",
        "         - x: tensor containing input features\n",
        "         - h: tensor containing the initial hidden state\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the next hidden state for each element in the batch\n",
        "\n",
        "        Shape:\n",
        "         - x: (batch_size, input_size)\n",
        "         - h: (batch_size, hidden_size)\n",
        "         - output: (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-1 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.RNNCell):\n",
        "        \"\"\"Copy the parameters from the given nn.RNNCell module.\"\"\"\n",
        "        # nn.RNNCell's weight_hh is W\n",
        "        # nn.RNNCell's weight_ih is U\n",
        "        # the sum of nn.RNNCell's bias_hh and bias_ih is b\n",
        "        self.W.data[:] = module.weight_hh\n",
        "        self.U.data[:] = module.weight_ih\n",
        "        if module.bias:\n",
        "            self.b.data[:] = module.bias_hh + module.bias_ih\n",
        "        else:\n",
        "            nn.init.zeros_(self.b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Write explanation here**"
      ],
      "metadata": {
        "id": "F6h6X1Lam7Dd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1-2 (Total 5 pts).  LSTM (Long Short-Term Memory)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/LSTM3-chain.png\" width=\"70%\"/>\n",
        "<br/>\n",
        "<em>Figure from <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>'s blog <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">post</a>.</em>\n",
        "</p>\n",
        "\n",
        "Recall the update equations of an LSTM.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    f^{(t)}\n",
        "    &=\\sigma(W_fh^{(t-1)}+U_fx^{(t)}+b_f)\\\\\n",
        "    g^{(t)}\n",
        "    &=\\sigma(W_gh^{(t-1)}+U_gx^{(t)}+b_g)\\\\\n",
        "    q^{(t)}\n",
        "    &=\\sigma(W_oh^{(t-1)}+U_ox^{(t)}+b_o)\\\\\n",
        "    \\tilde{s}^{(t)}\n",
        "    &=\\tanh(Wh^{(t-1)}+Ux^{(t)}+b)\\\\\n",
        "    s^{(t)}\n",
        "    &=f^{(t)}\\odot s^{(t-1)}+g^{(t)}\\odot\\tilde{s}^{(t)}\\\\\n",
        "    h^{(t)}\n",
        "    &=q^{(t)}\\odot \\tanh(s^{(t)})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Note that we use the $\\tanh$ nonlinearity to compute the next state in the fourth equation instead of the sigmoid nonlinearity as in the textbook.\n",
        "\n",
        "You have **two tasks** to do:  \n",
        "\n",
        "1. **Complete the skeleton code** given below using the equation above. You only need to rewrite the **`forward`** method. **Do not modify the methods `__init__` and `copy_from_torch`**. Feel free to implement additional helper functions if you need them.\n",
        "2. Explain how LSTM works **briefly** (e.g., meaning of the update equations, role of intermediate variables)"
      ],
      "metadata": {
        "id": "oGJil4NsY2lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file my_lstm_cell.py\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "\n",
        "class MyLSTMCell(nn.Module):\n",
        "    \"\"\" An long short-term memory (LSTM) cell.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.Wf = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.Uf = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.bf = nn.Parameter(torch.empty((hidden_size,)))\n",
        "        self.Wg = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.Ug = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.bg = nn.Parameter(torch.empty((hidden_size,)))\n",
        "        self.Wo = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.Uo = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.bo = nn.Parameter(torch.empty((hidden_size,)))\n",
        "        self.W = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.U = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.b = nn.Parameter(torch.empty((hidden_size,)))\n",
        "\n",
        "    def forward(\n",
        "        self, x: Tensor, h: Tensor, c: Tensor\n",
        "    ) -> tuple[Tensor, Tensor]:\n",
        "        \"\"\"Compute the next hidden state.\n",
        "\n",
        "        Parameters:\n",
        "         - x: tensor containing input features\n",
        "         - h: tensor containing the initial hidden state\n",
        "         - c: tensor containing the initial cell state\n",
        "\n",
        "        Returns:\n",
        "        tuple of two tensors (h', c')\n",
        "         - h': tensor containing the next hidden state for each element in the\n",
        "               batch\n",
        "         - c': tensor containing the next cell state for each element in the\n",
        "               batch\n",
        "\n",
        "        Shape:\n",
        "         - x: (batch_size, input_size)\n",
        "         - h: (batch_size, hidden_size)\n",
        "         - c: (batch_size, hidden_size)\n",
        "         - h': (batch_size, hidden_size)\n",
        "         - c': (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-2 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.LSTMCell) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.LSTMCell module.\"\"\"\n",
        "        # nn.LSTMCell's weight_hh is a concatenation of [Wg, Wf, W, Wo]\n",
        "        # nn.LSTMCell's weight_ih is a concatenation of [Ug, Uf, U, Uo]\n",
        "        # the sum of nn.LSTMCell's bias_ih and bias_hh is a concatenation of\n",
        "        # [bg, bf, b, bo]\n",
        "        params_dict = {\n",
        "            'W': module.weight_hh.chunk(4, dim=0),\n",
        "            'U': module.weight_ih.chunk(4, dim=0),\n",
        "        }\n",
        "        if module.bias:\n",
        "            params_dict['b'] = (module.bias_hh + module.bias_ih).chunk(4)\n",
        "        else:\n",
        "            nn.init.zeros_(self.bf)\n",
        "            nn.init.zeros_(self.bg)\n",
        "            nn.init.zeros_(self.bo)\n",
        "            nn.init.zeros_(self.b)\n",
        "        for prefix, params in params_dict.items():\n",
        "            for suffix, param in zip(('g', 'f', '', 'o'), params):\n",
        "                getattr(self, prefix + suffix).data[:] = param"
      ],
      "metadata": {
        "id": "c9PsGRBDYhug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Write explanation here**"
      ],
      "metadata": {
        "id": "ykcQpCTfnDuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1-3 (Total 5 pts). GRU (Gated Recurrent Units)\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/LSTM3-var-GRU.png\" width=\"70%\"/>\n",
        "<br/>\n",
        "<em>Figure adapted from <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>'s blog <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">post</a>.</em>\n",
        "</p>\n",
        "\n",
        "Recall the update equations for a GRU.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "    u^{(t)}\n",
        "    &=\\sigma(W_uh^{(t-1)}+U_ux^{(t)}+b_u)\\\\\n",
        "    r^{(t)}\n",
        "    &=\\sigma(W_rh^{(t-1)}+U_rx^{(t)}+b_r)\\\\\n",
        "    \\tilde{h}^{(t)}\n",
        "    &=\\tanh(r^{(t)}\\odot Wh^{(t-1)}+Ux^{(t)}+b)\\\\\n",
        "    h^{(t)}\n",
        "    &=u^{(t)}\\odot h^{(t-1)}+(1-u^{(t)}) \\tilde{h}^{(t)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Note that we use the $\\tanh$ nonlinearity to compute the next state in the third equation instead of the sigmoid nonlinearity as in the textbook. Also, the time index was modified to match the convention used in the LSTM update equations.\n",
        "\n",
        "You have **two tasks** to do:  \n",
        "\n",
        "1. **Complete the skeleton code** given below using the equation above. You only need to rewrite the **`forward`** method. **Do not modify the methods `__init__` and `copy_from_torch`**. Feel free to implement additional helper functions if you need them.\n",
        "2. Explain how GRU works **briefly** (e.g., meaning of the update equations, role of intermediate variables)"
      ],
      "metadata": {
        "id": "AD4WrsBFYLoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file my_gru_cell.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "\n",
        "\n",
        "class MyGRUCell(nn.Module):\n",
        "    \"\"\" An gated recurrent unit (GRU) cell.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.Wu = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.Uu = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.bu = nn.Parameter(torch.empty((hidden_size,)))\n",
        "        self.Wr = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.Ur = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.br = nn.Parameter(torch.empty((hidden_size,)))\n",
        "        self.W = nn.Parameter(torch.empty((hidden_size, hidden_size)))\n",
        "        self.U = nn.Parameter(torch.empty((hidden_size, input_size)))\n",
        "        self.b = nn.Parameter(torch.empty((hidden_size,)))\n",
        "\n",
        "    def forward(self, x: Tensor, h: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the next hidden state.\n",
        "\n",
        "        Parameters:\n",
        "         - x: tensor containing input features\n",
        "         - h: tensor containing the initial hidden state\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the next hidden state for each element in the batch\n",
        "\n",
        "        Shape:\n",
        "         - x: (batch_size, input_size)\n",
        "         - h: (batch_size, hidden_size)\n",
        "         - output: (batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-3 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.GRUCell) -> None:\n",
        "        # nn.GRUCell's weight_hh is a concatenation of [Wr, Wu, W]\n",
        "        # nn.GRUCell's weight_ih is a concatenation of [Ur, Uu, U]\n",
        "        # nn.GRUCell's bias_ih is a concatenation of [br, bu, b]\n",
        "        # MyGRUCell does not implement nn.GRUCell's bias_hh\n",
        "        params_dict = {\n",
        "            'W': module.weight_hh.chunk(3, dim=0),\n",
        "            'U': module.weight_ih.chunk(3, dim=0),\n",
        "        }\n",
        "        if module.bias:\n",
        "            if torch.any(module.bias_hh):\n",
        "                raise ValueError('MyGRUCell does not implement bias_hh')\n",
        "            params_dict['b'] = module.bias_ih.chunk(3)\n",
        "        else:\n",
        "            nn.init.zeros_(self.bu)\n",
        "            nn.init.zeros_(self.br)\n",
        "            nn.init.zeros_(self.b)\n",
        "        for prefix, params in params_dict.items():\n",
        "            for suffix, param in zip(('r', 'u', ''), params):\n",
        "                getattr(self, prefix + suffix).data[:] = param"
      ],
      "metadata": {
        "id": "joqMEuFYYLG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Write explanation here**"
      ],
      "metadata": {
        "id": "y4fNC26hnE9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1-4 (Total 5 pts). Unfolding Computational Graphs\n",
        "\n",
        "We have implemented modules that can compute the hidden state of the next state (\"green **A** block\" in the figure below). Now, we will implement a module that takes a whole sequence as an input and outputs the entire sequence of hidden states. See the right side of the figure below. Resulting module takes sequence $(x_0,\\ldots,x_t)$ as an input and returns sequence $(h_0,\\ldots,h_t)$ as an output.\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/RNN-unrolled.png\" width=\"70%\"/>\n",
        "<br/>\n",
        "<em>Figure adapted from <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>'s blog <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">post</a>.</em>\n",
        "</p>\n",
        "<br/>\n",
        "\n",
        "\n",
        "\n",
        "**Complete the skeleton code given below. You only need to rewrite the `forward` methods of `MyRNN`, `MyLSTM`, and `MyGRU`. Do not modify the methods `__init__` and `copy_from_torch` of the three classes. Feel free to implement additional helper functions if you need them.**"
      ],
      "metadata": {
        "id": "GZWVmOgNJdxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file unfold.py\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from my_gru_cell import MyGRUCell\n",
        "from my_lstm_cell import MyLSTMCell\n",
        "from my_rnn_cell import MyRNNCell\n",
        "\n",
        "\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    \"\"\" Applies a single layer RNN with tanh nonlinearity to an input sequence.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.cell = MyRNNCell(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, xs: Tensor, h0: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the hidden states for each timestep.\n",
        "\n",
        "        Parameters:\n",
        "         - xs: tensor containing the features of the input sequence\n",
        "         - h0: tensor containing the initial hidden state for the input sequence\n",
        "               batch\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the hidden features (h_t) for each t\n",
        "\n",
        "        Shape:\n",
        "         - xs: (sequence_length, batch_size, input_size)\n",
        "         - h0: (batch_size, hidden_size)\n",
        "         - output: (sequence_length, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-4 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.RNN, layer: int) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.RNN module.\"\"\"\n",
        "        # nn.RNN's weight_hh is W\n",
        "        # nn.RNN's weight_ih is U\n",
        "        # the sum of nn.RNN's bias_hh and bias_ih is b\n",
        "        self.cell.W.data[:] = getattr(module, f'weight_hh_l{layer}')\n",
        "        self.cell.U.data[:] = getattr(module, f'weight_ih_l{layer}')\n",
        "        if module.bias:\n",
        "            bias_hh = getattr(module, f'bias_hh_l{layer}')\n",
        "            bias_ih = getattr(module, f'bias_ih_l{layer}')\n",
        "            self.cell.b.data[:] = bias_hh + bias_ih\n",
        "        else:\n",
        "            nn.init.zeros_(self.cell.b)\n",
        "\n",
        "\n",
        "class MyLSTM(nn.Module):\n",
        "    \"\"\" Applies a single layer long short-term memory (LSTM) RNN to an input\n",
        "    sequence.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.cell = MyLSTMCell(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, xs: Tensor, h0: Tensor, c0: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the hidden states for each timestep.\n",
        "\n",
        "        Parameters:\n",
        "         - xs: tensor containing the features of the input sequence\n",
        "         - h0: tensor containing the initial hidden state for each element in\n",
        "               the input sequence\n",
        "         - c0: tensor containing the initial cell state for each element in the\n",
        "               input sequence\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the hidden features (h_t) for each t\n",
        "\n",
        "        Shape:\n",
        "         - xs: (sequence_length, batch_size, input_size)\n",
        "         - h0: (batch_size, hidden_size)\n",
        "         - c0: (batch_size, hidden_size)\n",
        "         - output: (sequence_length, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-4 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.LSTM, layer: int) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.LSTM module.\"\"\"\n",
        "        # nn.LSTM's weight_hh is a concatenation of [Wg, Wf, W, Wo]\n",
        "        # nn.LSTM's weight_ih is a concatenation of [Ug, Uf, U, Uo]\n",
        "        # the sum of nn.LSTM's bias_ih and bias_hh is a concatenation of\n",
        "        # [bg, bf, b, bo]\n",
        "        params_dict = {\n",
        "            'W': getattr(module, f'weight_hh_l{layer}').chunk(4, dim=0),\n",
        "            'U': getattr(module, f'weight_ih_l{layer}').chunk(4, dim=0),\n",
        "        }\n",
        "        if module.bias:\n",
        "            bias_hh = getattr(module, f'bias_hh_l{layer}')\n",
        "            bias_ih = getattr(module, f'bias_ih_l{layer}')\n",
        "            params_dict['b'] = (bias_hh + bias_ih).chunk(4)\n",
        "        else:\n",
        "            nn.init.zeros_(self.cell.bf)\n",
        "            nn.init.zeros_(self.cell.bg)\n",
        "            nn.init.zeros_(self.cell.bo)\n",
        "            nn.init.zeros_(self.cell.b)\n",
        "        for prefix, params in params_dict.items():\n",
        "            for suffix, param in zip(('g', 'f', '', 'o'), params):\n",
        "                getattr(self.cell, prefix + suffix).data[:] = param\n",
        "\n",
        "\n",
        "class MyGRU(nn.Module):\n",
        "    \"\"\" Applies a single layer gated recurrent unit (GRU) to an input sequence.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.cell = MyGRUCell(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, xs: Tensor, h0: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the hidden states for each timestep.\n",
        "\n",
        "        Parameters:\n",
        "         - xs: tensor containing the features of the input sequence\n",
        "         - h0: tensor containing the initial hidden state for the input sequence\n",
        "               batch\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the hidden features (h_t) for each t\n",
        "\n",
        "        Shape:\n",
        "         - xs: (sequence_length, batch_size, input_size)\n",
        "         - h0: (batch_size, hidden_size)\n",
        "         - output: (sequence_length, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-4 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.GRU, layer: int) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.GRU module.\"\"\"\n",
        "        # nn.GRU's weight_hh is a concatenation of [Wr, Wu, W]\n",
        "        # nn.GRU's weight_ih is a concatenation of [Ur, Uu, U]\n",
        "        # nn.GRU's bias_ih is a concatenation of [br, bu, b]\n",
        "        # MyGRUCell does not implement nn.GRU's bias_hh\n",
        "        params_dict = {\n",
        "            'W': getattr(module, f'weight_hh_l{layer}').chunk(3, dim=0),\n",
        "            'U': getattr(module, f'weight_ih_l{layer}').chunk(3, dim=0),\n",
        "        }\n",
        "        if module.bias:\n",
        "            if torch.any(getattr(module, f'bias_hh_l{layer}')):\n",
        "                raise ValueError('MyGRUCell does not implement bias_hh')\n",
        "            params_dict['b'] = getattr(module, f'bias_ih_l{layer}').chunk(3)\n",
        "        else:\n",
        "            nn.init.zeros_(self.cell.bu)\n",
        "            nn.init.zeros_(self.cell.br)\n",
        "            nn.init.zeros_(self.cell.b)\n",
        "        for prefix, params in params_dict.items():\n",
        "            for suffix, param in zip(('r', 'u', ''), params):\n",
        "                getattr(self.cell, prefix + suffix).data[:] = param"
      ],
      "metadata": {
        "id": "e_MAp14RQiOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1-5 (Total 5 pts). Stacking RNNs\n",
        "\n",
        "There are various ways to make an RNN deep. Here, we will implement the most popular way by feeding the hidden states of one RNN as an input to another RNN.\n",
        "\n",
        "<br/>\n",
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/mlrnn.png\" width=\"70%\"/>\n",
        "<br/>\n",
        "<em>Modified figure adapted from <a href=\"https://colah.github.io/about.html\">Christopher Olah</a>'s blog <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\">post</a>.</em>\n",
        "</p>\n",
        "<br/>\n",
        "\n",
        "**Complete the skeleton code given below. You only need to rewrite the `forward` methods of `MyDeepRNN`, `MyDeepLSTM`, and `MyDeepGRU`. Do not modify the methods `__init__` and `copy_from_torch` of the three classes. Feel free to implement additional helper functions if you need them.**"
      ],
      "metadata": {
        "id": "HiD9W3bcfPGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file stacking.py\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from unfold import MyGRU, MyLSTM, MyRNN\n",
        "\n",
        "\n",
        "class MyDeepRNN(nn.Module):\n",
        "    \"\"\" Applies a double layer RNN with tanh nonlinearity to an input sequence.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.layer0 = MyRNN(input_size, hidden_size)\n",
        "        self.layer1 = MyRNN(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, xs: Tensor, h0: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the hidden states for each timestep.\n",
        "\n",
        "        Parameters:\n",
        "         - xs: tensor containing the features of the input sequence\n",
        "         - h0: tensor containing the initial hidden state for the input sequence\n",
        "               batch\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the hidden features (h_t) for each t\n",
        "\n",
        "        Shape:\n",
        "         - xs: (sequence_length, batch_size, input_size)\n",
        "         - h0: (2, batch_size, hidden_size)\n",
        "         - output: (sequence_length, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-5 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.RNN) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.RNN module.\"\"\"\n",
        "        self.layer0.copy_from_torch(module, 0)\n",
        "        self.layer1.copy_from_torch(module, 1)\n",
        "\n",
        "\n",
        "class MyDeepLSTM(nn.Module):\n",
        "    \"\"\" Applies a double layer long short-term memory (LSTM) RNN to an input\n",
        "    sequence.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.layer0 = MyLSTM(input_size, hidden_size)\n",
        "        self.layer1 = MyLSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, xs: Tensor, h0: Tensor, c0: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the hidden states for each timestep.\n",
        "\n",
        "        Parameters:\n",
        "         - xs: tensor containing the features of the input sequence\n",
        "         - h0: tensor containing the initial hidden state for each element in\n",
        "               the input sequence\n",
        "         - c0: tensor containing the initial cell state for each element in the\n",
        "               input sequence\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the hidden features (h_t) for each t\n",
        "\n",
        "        Shape:\n",
        "         - xs: (sequence_length, batch_size, input_size)\n",
        "         - h0: (2, batch_size, hidden_size)\n",
        "         - c0: (2, batch_size, hidden_size)\n",
        "         - output: (sequence_length, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-5 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.LSTM) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.LSTMCell module.\"\"\"\n",
        "        self.layer0.copy_from_torch(module, 0)\n",
        "        self.layer1.copy_from_torch(module, 1)\n",
        "\n",
        "\n",
        "\n",
        "class MyDeepGRU(nn.Module):\n",
        "    \"\"\" Applies a single layer gated recurrent unit (GRU) to an input sequence.\n",
        "\n",
        "    Parameters:\n",
        "     - input_size(int): The number of expected features in the input x\n",
        "     - hidden_size(int): The number of features in the hidden state h\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int) -> None:\n",
        "        super().__init__()\n",
        "        self.layer0 = MyGRU(input_size, hidden_size)\n",
        "        self.layer1 = MyGRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, xs: Tensor, h0: Tensor) -> Tensor:\n",
        "        \"\"\"Compute the hidden states for each timestep.\n",
        "\n",
        "        Parameters:\n",
        "         - xs: tensor containing the features of the input sequence\n",
        "         - h0: tensor containing the initial hidden state for the input sequence\n",
        "               batch\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the hidden features (h_t) for each t\n",
        "\n",
        "        Shape:\n",
        "         - xs: (sequence_length, batch_size, input_size)\n",
        "         - h0: (2, batch_size, hidden_size)\n",
        "         - output: (sequence_length, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################# Part 1-5 #################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "    def copy_from_torch(self, module: nn.GRU) -> None:\n",
        "        \"\"\"Copy the parameters from the given nn.GRU module.\"\"\"\n",
        "        self.layer0.copy_from_torch(module, 0)\n",
        "        self.layer1.copy_from_torch(module, 1)"
      ],
      "metadata": {
        "id": "EDNXTNlykc3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1-6. Testing Everything\n",
        "We now write a bunch of scripts that can test your implementations. There is no need to understand what they do, as they have nothing to do with machine learning.  \n",
        "In this part, you have **nothing to implement**, but test all of your previous implementations. **Include the final result in your submission** so that we can check whether your models have passed all the tasks."
      ],
      "metadata": {
        "id": "PPn5l9XKTwLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%file conftest.py\n",
        "import pytest\n",
        "import torch\n",
        "\n",
        "\n",
        "@pytest.fixture(autouse=True, scope='session')\n",
        "def set_seed() -> None:\n",
        "    torch.random.manual_seed(42)"
      ],
      "metadata": {
        "id": "Bqk4RYzDXY7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_rnn.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from my_rnn_cell import MyRNNCell\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"input_size,hidden_size,batch_size\",\n",
        "     ((46, 25, 11), (49, 39, 28), (21, 42, 31)),\n",
        ")\n",
        "def test_rnn_cell(input_size: int, hidden_size: int, batch_size: int) -> None:\n",
        "    mycell = MyRNNCell(input_size, hidden_size)\n",
        "    cell = nn.RNNCell(input_size, hidden_size)\n",
        "    mycell.copy_from_torch(cell)\n",
        "    x = torch.rand((batch_size, input_size))\n",
        "    h = torch.rand((batch_size, hidden_size))\n",
        "    actual = mycell(x, h)\n",
        "    expected = cell(x, h)\n",
        "    torch.testing.assert_close(actual, expected)"
      ],
      "metadata": {
        "id": "TpPLLpFDT-XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_lstm.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from my_lstm_cell import MyLSTMCell\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"input_size,hidden_size,batch_size\",\n",
        "    ((46, 25, 11), (49, 39, 28), (21, 42, 31)),\n",
        ")\n",
        "def test_lstm_cell(input_size: int, hidden_size: int, batch_size: int) -> None:\n",
        "    mycell = MyLSTMCell(input_size, hidden_size)\n",
        "    cell = nn.LSTMCell(input_size, hidden_size)\n",
        "    mycell.copy_from_torch(cell)\n",
        "    x = torch.rand((batch_size, input_size))\n",
        "    h = torch.rand((batch_size, hidden_size))\n",
        "    c = torch.rand((batch_size, hidden_size))\n",
        "    actual = mycell(x, h, c)\n",
        "    expected = cell(x, (h, c))\n",
        "    torch.testing.assert_close(actual[0], expected[0])\n",
        "    torch.testing.assert_close(actual[1], expected[1])"
      ],
      "metadata": {
        "id": "68D3E8RYXg0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_gru.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from my_gru_cell import MyGRUCell\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize(\n",
        "    \"input_size,hidden_size,batch_size\",\n",
        "    ((46, 25, 11), (49, 39, 28), (21, 42, 31)),\n",
        ")\n",
        "def test_gru_cell(input_size: int, hidden_size: int, batch_size: int) -> None:\n",
        "    mycell = MyGRUCell(input_size, hidden_size)\n",
        "    cell = nn.GRUCell(input_size, hidden_size)\n",
        "    nn.init.zeros_(cell.bias_hh)\n",
        "    mycell.copy_from_torch(cell)\n",
        "    x = torch.rand((batch_size, input_size))\n",
        "    h = torch.rand((batch_size, hidden_size))\n",
        "    actual = mycell(x, h)\n",
        "    expected = cell(x, h)\n",
        "    torch.testing.assert_close(actual, expected)"
      ],
      "metadata": {
        "id": "OJsglqfQX0A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_unfold.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "from unfold import MyGRU, MyLSTM, MyRNN\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('module_type', ('rnn', 'lstm', 'gru'))\n",
        "@pytest.mark.parametrize(\n",
        "    \"input_size,hidden_size,batch_size,sequence_length\",\n",
        "    ((46, 25, 11, 49), (49, 39, 28, 44), (21, 42, 31, 26)),\n",
        ")\n",
        "def test_unfold(\n",
        "    module_type: str,\n",
        "    input_size: int,\n",
        "    hidden_size: int,\n",
        "    batch_size: int,\n",
        "    sequence_length: int,\n",
        ") -> None:\n",
        "    mymodule = {'rnn': MyRNN, 'lstm': MyLSTM, 'gru': MyGRU}[module_type](\n",
        "        input_size, hidden_size\n",
        "    )\n",
        "    module = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}[module_type](\n",
        "        input_size, hidden_size\n",
        "    )\n",
        "    if module_type == 'gru':\n",
        "        nn.init.zeros_(module.bias_hh_l0)\n",
        "    mymodule.copy_from_torch(module, 0)\n",
        "    x = torch.rand((sequence_length, batch_size, input_size))\n",
        "    h = torch.rand((batch_size, hidden_size))\n",
        "    if module_type == 'lstm':\n",
        "        c = torch.rand((batch_size, hidden_size))\n",
        "        actual = mymodule(x, h, c)\n",
        "        expected = module(x, (h.unsqueeze(0), c.unsqueeze(0)))\n",
        "    else:\n",
        "        actual = mymodule(x, h)\n",
        "        expected = module(x, h.unsqueeze(0))\n",
        "    torch.testing.assert_close(actual, expected[0])"
      ],
      "metadata": {
        "id": "tln9EDPzJc5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test_stacking.py\n",
        "import pytest\n",
        "import torch\n",
        "from torch import Tensor, nn\n",
        "\n",
        "from stacking import MyDeepGRU, MyDeepLSTM, MyDeepRNN\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('module_type', ('rnn', 'lstm', 'gru'))\n",
        "@pytest.mark.parametrize(\n",
        "    \"input_size,hidden_size,batch_size,sequence_length\",\n",
        "    ((46, 25, 11, 49), (49, 39, 28, 44), (21, 42, 31, 26)),\n",
        ")\n",
        "def test_unfold(\n",
        "    module_type: str,\n",
        "    input_size: int,\n",
        "    hidden_size: int,\n",
        "    batch_size: int,\n",
        "    sequence_length: int,\n",
        ") -> None:\n",
        "    mymodule = {\n",
        "        'rnn': MyDeepRNN, 'lstm': MyDeepLSTM, 'gru': MyDeepGRU\n",
        "    }[module_type](input_size, hidden_size)\n",
        "    module = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}[module_type](\n",
        "        input_size, hidden_size, 2\n",
        "    )\n",
        "    if module_type == 'gru':\n",
        "        nn.init.zeros_(module.bias_hh_l0)\n",
        "        nn.init.zeros_(module.bias_hh_l1)\n",
        "    mymodule.copy_from_torch(module)\n",
        "    x = torch.rand((sequence_length, batch_size, input_size))\n",
        "    h = torch.rand((2, batch_size, hidden_size))\n",
        "    if module_type == 'lstm':\n",
        "        c = torch.rand((2, batch_size, hidden_size))\n",
        "        actual = mymodule(x, h, c)\n",
        "        expected = module(x, (h, c))\n",
        "    else:\n",
        "        actual = mymodule(x, h)\n",
        "        expected = module(x, h)\n",
        "    torch.testing.assert_close(actual, expected[0])"
      ],
      "metadata": {
        "id": "lFwmCG9XnElw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can test your implementation by running the following command.\n",
        "\n",
        "```bash\n",
        "!pytest\n",
        "```\n",
        "If you need more information about the errors, try increasing the verbosity of `pytest` by setting the flags `-v` or `-vv`.\n",
        "\n",
        "Example:\n",
        "```bash\n",
        "!pytest -v\n",
        "!pytest -vv\n",
        "```"
      ],
      "metadata": {
        "id": "JN3rXwCYsl7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test all of your previous implementations**  \n",
        "You do not have to implement anything. Check whether your models have passed all 27 tasks. You don't have to care about testing time."
      ],
      "metadata": {
        "id": "Tm7VZ70SU8pD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "id": "x8bVSsOwq_H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we move on to Parts 2,3, and 4 that are independent to Part 1. Even if you weren't able to finish Part 1, you can still get points from Parts 2,3, and 4."
      ],
      "metadata": {
        "id": "U4q2Icu7Tu1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Enigma Decryption**"
      ],
      "metadata": {
        "id": "VtCWmL1_5FIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start, let us import required libraries and set the seeds for reproducibility.\n"
      ],
      "metadata": {
        "id": "g5yTRqHEsSkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "import re\n",
        "from string import ascii_lowercase\n",
        "\n",
        "from google.colab.data_table import DataTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam, Optimizer, RMSprop\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "import pdb\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "JJJIKyADyPkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "SEED = 7\n",
        "\n",
        "\n",
        "random.seed(SEED);\n",
        "np.random.seed(SEED);\n",
        "torch.manual_seed(SEED);"
      ],
      "metadata": {
        "id": "EPYtBoqU8tph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/The_Imitation_Game.jpg\" width=\"50%\"/>\n",
        "<br/>\n",
        "<em>Image from <a href=\"https://www.imdb.com/title/tt2084970\">IMDB</a>.</em>\n",
        "</p>\n",
        "\n",
        "Have you watched the movie [*The Imitation Game*](https://www.imdb.com/title/tt2084970)?\n",
        "For those who have not, *The Imitation Game* is a biographical film about the English mathematician and computer scientist [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing). During World War II, Germans heavily relied on the [enigma machine](https://en.wikipedia.org/wiki/Enigma_machine) to encipher all their top-secret messages. If the Allies could decipher these messages, they would be able to prepare for the German attacks. This would definitely bring huge strategic advantages to the Allies. However, the enigma machine adopted an extremely sophisticated encryption mechanism that produced messages notoriously difficult to decipher.\n",
        "\n",
        "In the movie, Turing creates an electro-mechanical machine called [bombe](https://en.wikipedia.org/wiki/Bombe) that automatically iterates over all possible enigma configurations while ruling out configurations that do not make sense. The remaining configurations were then handed over to cryptanalysts to be carefully inspected. Since the enigma machine has about $1.6\\times 10^{20}$ different possible configurations, reducing the set size to a manageable number was crucial for deciphering.\n",
        "\n",
        "In this project, we will try to decipher a bunch of secret messages ourselves! In fact, we will go one step further and create a machine that not only rules out impossible configurations but also detects the enigma configuration used to encipher the message **and** decipher the message by itself, without any help from human experts. Sounds extremely challenging, doesn't it? But no worries, GRU-ce Almighty will take care of it for you! Unfortunately, $1.6\\times 10^{20}$ types of configurations are way too much to handle, even for our GRUs. Therefore, we have simplified the problem as follows:\n",
        "\n",
        "1. We randomly selected **four** out of $1.6\\times 10^{20}$ possible configurations. You will only need to care about these four.\n",
        "\n",
        "2. We assume that the sentence structure and capitalization of the original message are preserved throughout the enciphering process. This can be very helpful in some instances. For example, if the cipher contains the word `B'a`, it would probably be `I'm`. If it has the word `Jzbvzrtn'o`, it would probably be a name followed by `'s`.\n",
        "\n",
        "Now, without further ado, let's get started!"
      ],
      "metadata": {
        "id": "f9NUUbdXSQ7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Data\n",
        "\n",
        "We first download the datasets to the `data` directory."
      ],
      "metadata": {
        "id": "HseRe7HfddA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data\n",
        "!wget -q --show-progress -P data https://media.githubusercontent.com/media/sisrel/EE214-Fall-2023/main/assets/train_classify.csv\n",
        "!wget -q --show-progress -P data https://media.githubusercontent.com/media/sisrel/EE214-Fall-2023/main/assets/train_decipher.csv\n",
        "!wget -q --show-progress -P data https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/valid_classify.csv\n",
        "!wget -q --show-progress -P data https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/valid_decipher.csv\n",
        "!wget -q --show-progress -P data https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/test_classify.csv\n",
        "!wget -q --show-progress -P data https://raw.githubusercontent.com/sisrel/EE214-Fall-2023/main/assets/test_decipher.csv"
      ],
      "metadata": {
        "id": "Yjz2DSTwdw7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classify dataset was created by modifying the [ROCStories (full five-sentence stories) winter 2017 set](https://cs.rochester.edu/nlp/rocstories/) [1].\n",
        "The ROCStories dataset consists of commonsense five-sentence stories.\n",
        "After fixing a few typos here and there, we preprocessed the data so that each sample does not exceed 120 characters.\n",
        "Each data was ciphered using the four different configurations. The number of samples for each split is shown in the following table. Obviously, they are all integer multiples of 4.\n",
        "\n",
        "| split | number of samples |\n",
        "|:-----:|:-----------------:|\n",
        "| train | 413536            |\n",
        "| valid | 40000             |\n",
        "| test  | 40000             |\n",
        "\n",
        "Here are the first 8 entries of the classify dataset.\n",
        "As you can see from the result below, the sentence structure and the punctuation marks were preserved during the enciphering process. Note that no sentence was split across multiple samples."
      ],
      "metadata": {
        "id": "pGrKQ3ZB0jy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataTable(pd.read_csv(\"./data/train_classify.csv\").head(8), include_index=False)"
      ],
      "metadata": {
        "id": "KORoWAWgzEYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the decipher dataset, we randomly sampled 64 letters to create one string, where each string was then enciphered with the chosen four different configurations. The number of samples for each split is shown in the following table. Obviously, they are all integer multiples of 4.\n",
        "\n",
        "| split | number of samples |\n",
        "|:-----:|:-----------------:|\n",
        "| train | 524288            |\n",
        "| valid | 32768             |\n",
        "| test  | 32768             |\n",
        "\n",
        "Here are the first 8 entries of the decipher dataset."
      ],
      "metadata": {
        "id": "7VVExmXl0tbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataTable(pd.read_csv(\"./data/train_decipher.csv\").head(8), include_index=False)"
      ],
      "metadata": {
        "id": "6W5TN3QJyQPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guideline  \n",
        "We provide a brief guideline on this project.\n",
        "\n",
        "* Enigma\n",
        " * In this task, we have four types of encryption rules. We will denote them by Type 0, 1, 2, and 3.\n",
        " * Each message is encrypted by a particular encryption rule chosen between these four types.\n",
        " * Note that the encryption mechanism of an enigma is history-dependent. Thus, we will use an RNN for decryption.\n",
        " * You do not need to understand the encryption mechanism of an enigma to finish this project. However, check out [this webpage](https://hackaday.com/2017/08/22/the-enigma-enigma-how-the-enigma-machine-worked/) if you are interested, as we found it very helpful.\n",
        "* Dataset\n",
        "  * Note that we use different datasets for classification and deciphering.\n",
        "  * Classify dataset\n",
        "    * Type of encryption, Encrypted message\n",
        "  * Decipher dataset\n",
        "    * Type of encryption, Original message, Encrypted message\n",
        "* Guideline for decryption\n",
        " * Given an encrypted message, our machine will decipher it in two steps\n",
        " * Step 1. Classification (**Part 2**)\n",
        "    + In this step, the classifier finds out which type of encryption rule has been used.\n",
        "    + This step is done by the `CLASSIFIER`, which determines the encryption type of a given text.\n",
        " * Step 2. Decyption (**Part 3**)\n",
        "    + In this step, the decipherer deciphers the message based on the encryption type obtained in step 1.\n",
        "    + This step is done by the `DECIPHERER`, which figures out the original message from a given encrypted message and its type (which will be obtained from the classifier).\n",
        " * You will design and train the classifier and the decipherer by yourselves.\n",
        " * Finally, you will implement a full decryptor using both the classifier and the decipherer (**Part 4**)"
      ],
      "metadata": {
        "id": "UgiYbP2EnCBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converter  \n",
        "As you can see, the dataset consists of text data. Since neural networks do not take text as input, we must convert them into numbers. The [`torchtext`](https://pytorch.org/text/stable/index.html) library provides a convenient tool called [`build_vocab_from_iterator`](https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator) for converting tokens to integers. Conventional natural language processing tasks utilize multi-character tokens because characters usually do not carry any meaning by themselves. However, the enigma problem does not have any natural multi-character tokenization scheme. So, we will just adopt character-based tokenization. Before starting, we need to figure out what kinds of characters are in the dataset. Note that the decipher datasets only contain lowercase alphabets, so we do not need to search through them."
      ],
      "metadata": {
        "id": "gvYRfEhMfWsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_characters():\n",
        "    characters = set()\n",
        "    with open(\"./data/train_classify.csv\", \"r\", encoding=\"utf-8\") as data:\n",
        "        reader = csv.reader(data)\n",
        "        for _, text in reader:\n",
        "            characters |= set(text)\n",
        "    return characters\n",
        "\n",
        "\n",
        "CHARACTERS = unique_characters()"
      ],
      "metadata": {
        "id": "tv0QwJcne3MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at what kinds of characters are in the classify dataset."
      ],
      "metadata": {
        "id": "JCW9GKVt4SL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print(characters: set[str]):\n",
        "    list_characters = list(characters)\n",
        "    list_characters.sort()\n",
        "    length = len(characters)\n",
        "    indices = list(range(0, length, 20)) + [length]\n",
        "    for i, j in zip(indices[:-1], indices[1:]):\n",
        "        print(\" \".join(list_characters[i:j]))\n",
        "\n",
        "\n",
        "\n",
        "pretty_print(CHARACTERS)"
      ],
      "metadata": {
        "id": "fx5dh63Q4f5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the character set, we build a character-to-integer converter."
      ],
      "metadata": {
        "id": "QNjpM6mGvxYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCABULARY_CLASSIFY = build_vocab_from_iterator(CHARACTERS)\n",
        "VOCABULARY_DECIPHER = build_vocab_from_iterator(ascii_lowercase)"
      ],
      "metadata": {
        "id": "xV8k-C0K8CSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Datasets\n",
        "Using the vocabulary converter, we create datasets. For the variable-length classify dataset, we pad the data with zeros."
      ],
      "metadata": {
        "id": "uu9vffNM8c9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_classify_dataset(name: str) -> TensorDataset:\n",
        "    label_list: list[int] = []\n",
        "    text_list: list[str] = []\n",
        "    length_list: list[int] = []\n",
        "    with open(f\"./data/{name}_classify.csv\", \"r\", encoding=\"utf-8\") as data:\n",
        "        reader = csv.reader(data)\n",
        "        for index, (label, text) in enumerate(reader):\n",
        "            if index == 0:\n",
        "                continue\n",
        "            label_list.append(int(label))\n",
        "            text_list.append(text)\n",
        "            length_list.append(len(text))\n",
        "    labels = torch.as_tensor(label_list, dtype=torch.int64)\n",
        "    lengths = torch.as_tensor(length_list, dtype=torch.int64)\n",
        "    num_samples = lengths.size(0)\n",
        "    max_length = torch.max(lengths)\n",
        "    sequences_array = np.zeros((num_samples, max_length), dtype=np.int64)\n",
        "    for index, text in enumerate(text_list):\n",
        "        sequences_array[index, : len(text)] = VOCABULARY_CLASSIFY(list(text))\n",
        "    sequences = torch.as_tensor(sequences_array, dtype=torch.int64)\n",
        "    return TensorDataset(sequences, lengths, labels)\n",
        "\n",
        "\n",
        "CLASSIFY_DATASETS = {\n",
        "    \"train\": create_classify_dataset(\"train\"),\n",
        "    \"valid\": create_classify_dataset(\"valid\"),\n",
        "    \"test\": create_classify_dataset(\"test\"),\n",
        "}"
      ],
      "metadata": {
        "id": "UkQuqLue8lBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We repeat the same process for the decipher dataset. Since every sample has a length of 64, there's no need for padding."
      ],
      "metadata": {
        "id": "0my_fxbFRjAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_decipher_dataset(name: str) -> TensorDataset:\n",
        "    label_list: list[int] = []\n",
        "    original_list: list[list[int]] = []\n",
        "    enciphered_list: list[list[int]] = []\n",
        "    with open(f\"./data/{name}_decipher.csv\", \"r\", encoding=\"utf-8\") as data:\n",
        "        reader = csv.reader(data)\n",
        "        for index, (label, original, enciphered) in enumerate(reader):\n",
        "            if index == 0:\n",
        "                continue\n",
        "            label_list.append(int(label))\n",
        "            original_list.append(VOCABULARY_DECIPHER(list(original)))\n",
        "            enciphered_list.append(VOCABULARY_DECIPHER(list(enciphered)))\n",
        "    labels = torch.as_tensor(label_list, dtype=torch.int64)\n",
        "    originals = torch.as_tensor(original_list, dtype=torch.int64)\n",
        "    enciphereds = torch.as_tensor(enciphered_list, dtype=torch.int64)\n",
        "    return TensorDataset(labels, enciphereds, originals)\n",
        "\n",
        "\n",
        "DECIPHER_DATASETS = {\n",
        "    \"train\": create_decipher_dataset(\"train\"),\n",
        "    \"valid\": create_decipher_dataset(\"valid\"),\n",
        "    \"test\": create_decipher_dataset(\"test\"),\n",
        "}"
      ],
      "metadata": {
        "id": "0sUK1buOtyNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparation  \n",
        "We now define helper functions for training and testing. They are implemented in a model-independent manner to use them on both the classifier and the decipherer."
      ],
      "metadata": {
        "id": "uxIFBRlc7b3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, loader: DataLoader) -> float:\n",
        "    model.eval()\n",
        "    total = correct = 0\n",
        "    for inputs0, inputs1, targets in loader:\n",
        "        targets = targets.to(device=DEVICE)\n",
        "        outputs = model(inputs0, inputs1)\n",
        "        predictions = torch.argmax(outputs, dim=-1)\n",
        "        correct += (predictions == targets).sum().item()\n",
        "        total += targets.numel()\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def train(\n",
        "    model: nn.Module,\n",
        "    datasets: dict[str, TensorDataset],\n",
        "    optimizer_type: str,\n",
        "    learning_rate: float,\n",
        "    batch_size: int,\n",
        "    max_epochs: int,\n",
        "    **kwargs\n",
        ") -> tuple[list[float], list[float]]:\n",
        "    trainloader = DataLoader(\n",
        "        datasets[\"train\"], shuffle=True, batch_size=batch_size, drop_last=True\n",
        "    )\n",
        "    validloader = DataLoader(\n",
        "        datasets[\"valid\"], shuffle=False, batch_size=512, drop_last=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer: Optimizer\n",
        "    match optimizer_type:\n",
        "        case \"Adam\":\n",
        "            optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "        case \"RMSprop\":\n",
        "            optimizer = RMSprop(model.parameters(), lr=learning_rate)\n",
        "        case other:\n",
        "            raise NotImplementedError(optimizer_type)\n",
        "    steps = 0\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    for epoch in trange(max_epochs, **kwargs):\n",
        "        model.train()\n",
        "        for inputs0, inputs1, targets in tqdm(\n",
        "            trainloader, desc=f\"Epoch {epoch}\", leave=False\n",
        "        ):\n",
        "            targets = targets.to(device=DEVICE)\n",
        "            predictions = model(inputs0, inputs1)\n",
        "            predictions = predictions.view(-1, predictions.size(-1))\n",
        "            targets = targets.view(-1)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(predictions, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            steps += 1\n",
        "            if steps % 100 == 0:\n",
        "                loss_list.append(loss.item())\n",
        "                accuracy_list.append(evaluate(model, validloader))\n",
        "                model.train()\n",
        "    return loss_list, accuracy_list\n",
        "\n",
        "\n",
        "def test(task: str, model: nn.Module, datasets: dict[str, TensorDataset]):\n",
        "    loader = DataLoader(\n",
        "        datasets[\"test\"], shuffle=False, batch_size=512, drop_last=False\n",
        "    )\n",
        "    accuracy = evaluate(model, loader)\n",
        "    print(f\"{task} Test Accuracy: {accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "kMo5acEeu2eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also define helper functions for plotting the results."
      ],
      "metadata": {
        "id": "DUUX5j7Xzd0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_frame(losses: list[float], accuracies: list[float]) -> pd.DataFrame:\n",
        "    frame = pd.DataFrame(\n",
        "        {\n",
        "            \"Epoch\": np.arange(len(losses)),\n",
        "            \"Loss\": losses,\n",
        "            \"Validation Accuracy\": accuracies,\n",
        "        }\n",
        "    )\n",
        "    frame = pd.melt(\n",
        "        frame, id_vars=[\"Epoch\"], var_name=\"Type\", value_name=\"Value\"\n",
        "    )\n",
        "    return frame\n",
        "\n",
        "\n",
        "def plot(title: str, losses: list[float], accuracies: list[float]):\n",
        "    frame = pd.DataFrame(\n",
        "        {\n",
        "            \"Steps\": np.arange(1, len(losses) + 1) * 100,\n",
        "            \"Loss\": losses,\n",
        "            \"Validation Accuracy\": accuracies,\n",
        "        }\n",
        "    )\n",
        "    frame = pd.melt(\n",
        "        frame, id_vars=[\"Steps\"], var_name=\"Type\", value_name=\"Value\"\n",
        "    )\n",
        "    grid = sns.relplot(\n",
        "        frame,\n",
        "        x=\"Steps\",\n",
        "        col=\"Type\",\n",
        "        y=\"Value\",\n",
        "        facet_kws={\"sharey\": False},\n",
        "        kind=\"line\",\n",
        "    )\n",
        "    grid.set_titles(col_template=\"{col_name}\")\n",
        "    grid.set_axis_labels(\"Steps\", \"\")\n",
        "    grid.set(title=title)"
      ],
      "metadata": {
        "id": "WhSv-hHx4gym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2 (Total 30 pts)**. Enigma decryption: Classifier"
      ],
      "metadata": {
        "id": "JOBXF1fVPOZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guideline\n",
        "\n",
        "We will provide some guidelines  \n",
        "* **Notice** You can add or delete what you want in `__init__()`\n",
        "* Goal  \n",
        "  * The resulting `CLASSIFIER` will take enciphered sequences and their lengths as inputs and return 4-dimensional vectors.\n",
        "  * We recommend you aim for at least 95% accuracy on the test set, as we will use the learned classifier in the final part of the project.\n",
        "* embedding  \n",
        " * Since we can not calculate with characters (e.g., 'a'), we need to convert them into numeric form, vector.\n",
        " * Typical procedure: character $\\to$ integer $\\to$ vector  \n",
        " * character $\\to$ integer: find from lookup table `VOCABULARY_CLASSIFY` (we already made it)\n",
        " * integer $\\to$ vector: find from lookup table `self.embedding` (we provided it below)\n",
        " * Use `self.embedding` to convert the texts into vectors (whose dimension is `embedding_dim`)  \n",
        " For more details, see [`here`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding).\n",
        "\n",
        "| character | integer | vector (`embedding_dim` dimension) |  \n",
        "|:-------------------:|:----------------------:|:----------------:|  \n",
        "| a                 | 0                   | e.g. [0.12, 3.35, $\\ldots$, 1.52]                |  "
      ],
      "metadata": {
        "id": "-_lM9GwcB9uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Models\n",
        "\n",
        "As mentioned above, we adopt GRU as our base model. The table below shows the architecture parameters we've tested. You can change these parameters or even try other architectures like LSTM.\n",
        "\n",
        "\n",
        "| Embedding Dimension | Hidden State Dimension | Number of Layers |\n",
        "|:-------------------:|:----------------------:|:----------------:|\n",
        "| 128                 | 2048                   | 1                |"
      ],
      "metadata": {
        "id": "mTaURys2uPq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\" Detects the encryption type of the input message.\n",
        "\n",
        "    Parameters:\n",
        "     - embedding_dim(int): The size of each embedding vector.\n",
        "     - hidden_size(int): The number of features in the hidden state h.\n",
        "     - num_layers(int):  The number of recurrent layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim: int, hidden_size: int, num_layers: int):\n",
        "        super().__init__()\n",
        "        ############################################\n",
        "        ################## Part 2 ##################\n",
        "        # Feel free to modify this part if you want.\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=len(VOCABULARY_CLASSIFY), embedding_dim=embedding_dim\n",
        "        )\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.linear = nn.Linear(in_features=hidden_size, out_features=4)\n",
        "        ############################################\n",
        "        ############################################\n",
        "        self.to(device=DEVICE)\n",
        "\n",
        "    def forward(\n",
        "        self, texts: torch.Tensor, lengths: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\" Detects the encryption type of the input message.\n",
        "\n",
        "        Parameters:\n",
        "         - texts: integer tensor containing the index of each token in the input\n",
        "                  message\n",
        "         - lengths: integer tensor containing the number of tokens for each\n",
        "                    input message\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the logits of the classifcation result for each\n",
        "        message\n",
        "\n",
        "        Shape:\n",
        "         - texts: (batch_size, max_sequence_length)\n",
        "         - lengths: (batch_size,)\n",
        "         - output: (batch_size, 4)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################## Part 2 ##################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "\n",
        "CLASSIFIER = Classifier(128, 2048, 1)"
      ],
      "metadata": {
        "id": "o3JIXnAEuLG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Models  \n",
        "\n",
        "We train the classifier using the following hyperparameters. You may change them if you want.\n",
        "\n",
        "| Task | optimizer | learning rate | batch size | max epochs |\n",
        "|:-:|:-:|:-:|:-:|:-:|\n",
        "| Classify | [`torch.optim.RMSprop`](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) | 0.0001 | 512 | 3 |"
      ],
      "metadata": {
        "id": "zA6XarUyvOlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSIFY_LOSSES, CLASSIFY_ACCURACIES = train(\n",
        "    model=CLASSIFIER,\n",
        "    datasets=CLASSIFY_DATASETS,\n",
        "    optimizer_type=\"RMSprop\",\n",
        "    learning_rate=0.0001,\n",
        "    batch_size=512,\n",
        "    max_epochs=3,\n",
        ")"
      ],
      "metadata": {
        "id": "jSYuVN0Ovkux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results  \n",
        "Let us plot how the loss and the validation accuracy change throughout the training process."
      ],
      "metadata": {
        "id": "VcTj4mgU4w_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(\"Classify\", CLASSIFY_LOSSES, CLASSIFY_ACCURACIES)"
      ],
      "metadata": {
        "id": "3MaMnD4J4ugX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we evaluate the models on the test set."
      ],
      "metadata": {
        "id": "7q8QCiik46RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(\"Classify\", CLASSIFIER, CLASSIFY_DATASETS)"
      ],
      "metadata": {
        "id": "LKyP_oop4Ssp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3 (Total 30 pts)**. Enigma decryption: Decipherer"
      ],
      "metadata": {
        "id": "iBiBouUyt68X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Models\n",
        "\n",
        "As mentioned above, we adopt GRU as our base model. The table below shows the architecture parameters we've tested. You can change these parameters or even try other architectures like LSTM.\n",
        "\n",
        "\n",
        "| Hidden State Dimension | Number of Layers |\n",
        "|:----------------------:|:----------------:|\n",
        "| 2048                   | 1                |"
      ],
      "metadata": {
        "id": "M_HDT2_A8WTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Guideline\n",
        "\n",
        "* Goal  \n",
        "  * The `DECIPHERER` takes labels(i.e., enigma configuration types) and the ciphers as inputs and returns the decrypted message.\n",
        "  * This task is slightly more challenging than the previous one. Think carefully about how to incorporate the label information together with the input tokens. One possible way would be to use different initial states for each configuration type while training those initial states together with the rest of the model.\n",
        "  * We recommend you aim for at least 95% accuracy on the test set, as we will use the learned decipherer in the final part of the project."
      ],
      "metadata": {
        "id": "I6WF7UFEXR2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decipherer(nn.Module):\n",
        "    \"\"\" Decipher the input message based on the given enigma configuration.\n",
        "\n",
        "    Parameters:\n",
        "     - hidden_size(int): The number of features in the hidden state h.\n",
        "     - num_layers(int):  The number of recurrent layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int, num_layers: int):\n",
        "        super().__init__()\n",
        "        ############################################\n",
        "        ################## Part 3 ##################\n",
        "        # Feel free to modify this part if you want.\n",
        "        num_characters = len(VOCABULARY_DECIPHER)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.text_embedding = nn.Embedding.from_pretrained(\n",
        "            torch.eye(num_characters, dtype=torch.float32), freeze=True\n",
        "        )\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=num_characters,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features=hidden_size, out_features=num_characters\n",
        "        )\n",
        "        ############################################\n",
        "        ############################################\n",
        "        self.to(device=DEVICE)\n",
        "\n",
        "    def forward(\n",
        "        self, labels: torch.Tensor, texts: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\" Decipher the input message based on the given enimga configuration\n",
        "        type.\n",
        "\n",
        "        Parameters:\n",
        "         - labels: integer tensor containing the configuration type of each\n",
        "                   input message\n",
        "         - texts: integer tensor containing the index of each token in the input\n",
        "                  message\n",
        "\n",
        "        Returns:\n",
        "        tensor containing the logits for each deciphered token\n",
        "\n",
        "        Shape:\n",
        "         - labels: (batch_size,)\n",
        "         - texts: (batch_size, sequence_length)\n",
        "         - output: (batch_size, sequence_length, vocabulary_size)\n",
        "        \"\"\"\n",
        "        ############################################\n",
        "        ################## Part 3 ##################\n",
        "\n",
        "        ############################################\n",
        "        ############################################\n",
        "\n",
        "\n",
        "DECIPHERER = Decipherer(2048, 1)"
      ],
      "metadata": {
        "id": "eqo0_k90C_f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Models\n",
        "\n",
        "We train the models using the following hyperparameters. You may change these parameters.\n",
        "\n",
        "| Task | optimizer | learning rate | batch size | max epochs |\n",
        "|:-:|:-:|:-:|:-:|:-:|\n",
        "| Decipher | [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) | 0.0001 | 512 | 5 |"
      ],
      "metadata": {
        "id": "VmeaoRVpsSQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DECIPHER_LOSSES, DECIPHER_ACCURACIES = train(\n",
        "    model=DECIPHERER,\n",
        "    datasets=DECIPHER_DATASETS,\n",
        "    optimizer_type=\"Adam\",\n",
        "    learning_rate=0.0001,\n",
        "    batch_size=512,\n",
        "    max_epochs=5,\n",
        ")"
      ],
      "metadata": {
        "id": "T68REIcntVkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results  \n",
        "Let us plot how the loss and the validation accuracy change throughout the training process."
      ],
      "metadata": {
        "id": "1aHfvwIN-DLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(\"Decipher\", DECIPHER_LOSSES, DECIPHER_ACCURACIES)"
      ],
      "metadata": {
        "id": "8BCEh_Vy-9_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we evaluate the models on the test set."
      ],
      "metadata": {
        "id": "reo_TR-y_VZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test(\"Decipher\", DECIPHERER, DECIPHER_DATASETS)"
      ],
      "metadata": {
        "id": "fiLwOy-q_aDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4 (Total 15 pts)**. Enigma decryptor"
      ],
      "metadata": {
        "id": "KiLjduNxAYx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you combine your classifier and decipherer to make a full decryptor. You will be given seven ciphers whose original sentences are movie quotes. You need to find out the title of the movie that the quote appears. If you are not familiar with movies, feel free to google them.   \n",
        "\n",
        "* **Complete the table below**. Consult the sample row we provided for what to write.\n",
        "* You will get 3 points for each correct movie title.\n",
        "* However, we will also check the text you deciphered, so do not just randomly guess titles and hope they are correct."
      ],
      "metadata": {
        "id": "jj8jd95XR_Y1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Index | Cipher | Decrypted | Enigma Configuration Type | Your guess on the original quote | Movie title |\n",
        "|:-:|:-|:-|:-:|:-|:-:|\n",
        "| Sample | H'xz ks qtvy. | I'll be bacx. | 0 | I'll be back. | The Terminator |\n",
        "| 0 | Nhazchs sp'd zfq aau pfph. Ju'b v zyingb qlcylcup,</br>l plrzmach nkeunhngy. O fcdi iunkse. | (your answer) | (your answer) | (your answer) | (your answer) |\n",
        "| 1 | Vuufj gpq lfan fkui? Yodpng. | (your answer) | (your answer) | (your answer) | (your answer) |\n",
        "| 2 | Sjmqs ryso. Uresg hai ahf, taao. Ebwj mvep ezzkm</br>bgejftynfalvc. | (your answer) | (your answer) | (your answer) | (your answer) |\n",
        "| 3 | Ll djp ue lvljmm bprt fvgg rjmc bpcbg. Wznm, lfeo</br>kojblvp zml whwdj qh dmg wqdcs. | (your answer) | (your answer) | (your answer) | (your answer) |\n",
        "| 4 | Pwckzox, vlbf apxvt, Yipzakal, Eyehsrp, aoenveorv</br>Cgp Vca qb, oo'l cwft etxvem. | (your answer) | (your answer) | (your answer) | (your answer) |\n"
      ],
      "metadata": {
        "id": "za1mEVSuR_Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Guideline  \n",
        "* Properly **combine your classifier and decipherer** trained in the part 2, 3.\n",
        " * We offered a diagram for the overall structure below.\n",
        "* Properly **process the data**.\n",
        "  * The form of text that the classifier and the decipherer take as input are **different**. You should appropriately process the ciphers before giving them as input.\n",
        "    * Classifier dataset: Contains capital letters, punctuation marks, and spaces (e.g., \"Ip mffsyjrm oi'q tbip.\")\n",
        "    * Decipherer dataset: Contains only lowercase letters. (e.g., \"ipmffsyjrmoiqtbip\")\n",
        "  * Recall that we trained the decipherer using a dataset with texts of length 64. How does this fact affect the decryption of ciphers that are longer than 64 characters? How would you deal with this issue? (You do not have to answer it. This might not be a big problem.)\n",
        "\n",
        "</br>\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/sisrel/EE214-Fall-2023/blob/main/assets/EEpj3.png?raw=true\" alt=\"EEpj3\">\n",
        "</p>"
      ],
      "metadata": {
        "id": "j4T78hLbR-hi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the decryptor  \n",
        "\n",
        "* **Complete the `decrypt` function.**\n",
        "* You may retrain the classifier and the decipherer with different hyperparameters (e.g., model size, batch size, max epochs, etc) if you want.\n",
        "* However, **all your codes must be runnable on colab**.  \n",
        "* You can use the methods [`VOCABULARY_DECIPHER.lookup_token`](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.lookup_token) or [`VOCABULARY_DECIPHER.lookup_tokens`](https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab.lookup_tokens) to convert integers back to characters. Feel free to use other methods if you want."
      ],
      "metadata": {
        "id": "1l0lcChB-HLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DO NOT EDIT THIS CELL\n",
        "\n",
        "CIPHERS = [\n",
        "    \"Nhazchs sp'd zfq aau pfph. Ju'b v zyingb qlcylcup, l plrzmach nkeunhngy. O fcdi iunkse.\",\n",
        "    \"Vuufj gpq lfan fkui? Yodpng.\",\n",
        "    \"Sjmqs ryso. Uresg hai ahf, taao. Ebwj mvep ezzkm bgejftynfalvc.\",\n",
        "    \"Ll djp ue lvljmm bprt fvgg rjmc bpcbg. Wznm, lfeo kojblvp zml whwdj qh dmg wqdcs.\",\n",
        "    \"Pwckzox, vlbf apxvt, Yipzakal, Eyehsrp, aoenveorv Cgp Vca qb, oo'l cwft etxvem.\",\n",
        "]\n",
        "\n",
        "\n",
        "### You may use this function if you want\n",
        "def finalize(cipher: str, decrypted: str) -> str:\n",
        "    \"\"\" Post process the decrypted message\n",
        "\n",
        "    Add spaces and punctuation marks and capitalize certain letters to match the\n",
        "    sentence structure of the given cipher.\n",
        "\n",
        "    Parameters:\n",
        "     - cipher: the cipher\n",
        "     - decrypted: the output of the decipherer converted to lowercase letters\n",
        "\n",
        "    Returns:\n",
        "    Decrypted message that has an identical sentence structure with the cipher.\n",
        "    \"\"\"\n",
        "    expected_length = len(re.sub(r\"[^a-zA-Z]\", \"\", cipher))\n",
        "    if expected_length != len(decrypted):\n",
        "        raise ValueError(\n",
        "            f\"Expected a string of length: {expected_length}. \"\n",
        "            f\"Received a string of length: {len(decrypted)}.\"\n",
        "        )\n",
        "\n",
        "    output = \"\"\n",
        "    index = 0\n",
        "\n",
        "    for character in cipher:\n",
        "      if character.isalpha():\n",
        "        if character.isupper():\n",
        "            output += decrypted[index].upper()\n",
        "        else:\n",
        "            output += decrypted[index]\n",
        "        index += 1\n",
        "      else:\n",
        "        output += character\n",
        "    return output"
      ],
      "metadata": {
        "id": "H-38XPpOWqyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to import additional libraries if you need them\n",
        "\n",
        "\n",
        "def decrypt(cipher: str) -> tuple[int, str]:\n",
        "    \"\"\" Decrypt the cipher.\n",
        "\n",
        "    Parameters:\n",
        "     - cipher: the string you need to decrypt\n",
        "\n",
        "    Returns:\n",
        "    tuple of an integer and a string (type, decrypted)\n",
        "     - type: Enigma configuration type used to encrypt the cipher.\n",
        "             Should be one of 0, 1, 2, and 3.\n",
        "     - decrypted: Decrypted message.\n",
        "    \"\"\"\n",
        "    ############################### Part 4 ###############################\n",
        "    ################# Freely design your decryptor here ##################\n",
        "\n",
        "    ######################################################################\n",
        "    ######################################################################"
      ],
      "metadata": {
        "id": "jyjYM2AdWqvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CELL\n",
        "\n",
        "def print_decrypt_results():\n",
        "    for index, cipher in enumerate(CIPHERS):\n",
        "        if index > 0:\n",
        "            print()\n",
        "        enigma_type, decrypted = decrypt(cipher)\n",
        "        print(f\"Cipher {index}: \", cipher)\n",
        "        print(\"Decrypted:\", decrypted)\n",
        "        print(\"Enigma configuration type:\", enigma_type)\n",
        "\n",
        "\n",
        "print_decrypt_results()"
      ],
      "metadata": {
        "id": "xDGidhGBWZQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References  \n",
        "[1] \"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories\". Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli and James Allen. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), 2016"
      ],
      "metadata": {
        "id": "9EONsmFame-2"
      }
    }
  ]
}